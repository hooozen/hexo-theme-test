<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
<meta name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=2.0"/>
<meta name="theme-color" content="theme.android_chrome_color"/>
<meta name="generator" content="Hexo"/>
<title>Lei Li</title>

  <link rel="apple-touch-icon"
        sizes="180x180"
        href="/academic/images/apple-touch-icon-academic.png"/>


  <link rel="icon"
        type="image/png"
        sizes="32x32"
        href="/academic/images/apple-touch-icon-academic.png"/>


  <link rel="icon"
        type="image/png"
        sizes="16x16"
        href="/academic/images/apple-touch-icon-academic.png"/>

<link rel="stylesheet"
      type="text/css"
      href="/academic/css/main.css"/>

  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"/>

<script class="academic-config" data-name="main" type="application/json">{"comments":{"index":true,"enable":true},"root":"/academic/"}</script>
<script src="/academic/js/config.js"></script>

    <script class="academic-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://theme.hozen.site/academic/2023/07/03/feat-%E5%85%AC%E5%BC%8F%E6%B5%8B%E8%AF%95/","path":"2023/07/03/feat-公式测试/","title":"数学公式测试"}</script>

    
  </head>
  <body itemscope itemtype="http://schema.org/WebPage">
    <div class="container">
      <div class="header" itemscope itemtype="http://schema.org/WPHeader">
        <div class="header-inner"><div class="nav">
  <input id="menuState" style="display: none" type="checkbox" />
<label class="nav-menu-icon" id="menuToggle" for="menuState">
  <div class="nav-menu-bread nav-menu-bread__top">
    <div class="nav-menu-crust nav-menu-crust__top"></div>
  </div>
  <div class="nav-menu-bread nav-menu-bread__bottom">
    <div class="nav-menu-crust nav-menu-crust__bottom"></div>
  </div>
</label>

  <div class="nav-logo"
       style="background-image: url(/academic/images/logo-academic.png)"></div>
  <div class="nav-title">Lilei's research</div>
  <div class="nav-items-outer" id="menu">
    <div class="nav-items">
      <a class="nav-item" href="/academic/./">Home</a>
      <a class="nav-item" href="/academic/publications/">Publications</a>
      
        <a class="nav-item" href="/academic/blog/">Blog</a>
      
    </div>
  </div>
</div>


</div>
      </div>
      <div class="main">
        
  <div class="main__content">
    <div class="main__content-inner">
      <div class="post-block">
  <div class="post-head">
    <div class="post-title">数学公式测试</div>
    <div class="post-info">
      <div class="post-author">Lei Li</div>
      <div class="post-info__sep"></div>
      <div class="post-updated updated">2023-07-04 02:12:11</div>
      
  <div class="post-tags">
    
      <a class="post-tag" href="/academic/tags/math/">#math</a>
    
  </div>


    </div>
  </div>
  <div class="outline-anchor">
    <div id="outline" class="outline">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98"><span class="toc-text">问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%9D%E8%B7%AF"><span class="toc-text">思路</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">梯度下降</span></a></li></ol>
    </div>
  </div>
  <article class="pos-main">
    <p><em>原文来自 &lt;www.hozen.site&gt; [机器学习基础-“逻辑”回归
(logistic regression) 的数学原理](<a
target="_blank" rel="noopener" href="https://www.hozen.site/archives/33/"
class="uri">https://www.hozen.site/archives/33/</a></em></p>
<p>通过使用 logistic regression
处理一个二分类问题来简要分析其中的数学原理。 <span id="more"></span></p>
<h3 id="问题">问题</h3>
<p>假设我们有这样一个问题，根据医学图像来判断病人的肿瘤为恶性还是良性，这就是一个二分类问题。</p>
<p>相对于回归问题，分类问题的输出值为离散值。进一步地，二分类的输出值只有两个，通常称其为“正类”和“负类”，也可以用
1 和 0 分别表示。</p>
<h3 id="思路">思路</h3>
<p>同回归问题一样，我们希望得到假设 <span
class="math inline">\(h_\boldsymbol\theta(\boldsymbol{x})\)</span>
对应的代价函数 <span
class="math inline">\(J(\boldsymbol\theta)\)</span>，并通过最小化代价函数来确定合适的参数值
<span class="math inline">\(\boldsymbol\theta\)</span>。</p>
<p>下面推导代价函数：</p>
<p>先明确一些记法：粗体字母代表向量，一般使用 <span
class="math inline">\(\boldsymbol{x}\)</span>
表示一个样例的特征向量，<span
class="math inline">\(\boldsymbol{\theta}\)</span> 表示参数向量，实数
<span class="math inline">\(y\in\{0,1\}\)</span>
表示该样例的标记。当处理多个样例时，使用 <span
class="math inline">\(\boldsymbol{x^{(j)}}\)</span> 表示第 <span
class="math inline">\(j\)</span> 个样例的特征向量, 对应的标记为 <span
class="math inline">\(y^{(j)}\)</span>。向量 <span
class="math inline">\(\boldsymbol{y}\)</span>
表示多个样例的标记向量。具体的：</p>
<p><span class="math display">\[
\boldsymbol{x^{(j)}}=\left[\begin{matrix}
x^{(j)}_1 \\
x^{(j)}_2 \\
\vdots \\
x^{(j)}_n \\
\end{matrix}\right],\
\boldsymbol\theta=\left[\begin{matrix}
\theta_0 \\
\theta_1 \\
\vdots \\
\theta_n \\
\end{matrix}\right],\
\boldsymbol{y} = \left[\begin{matrix}
y^{(1)}\\
y^{(2)}\\
\vdots \\
y^{(m)}\\
\end{matrix}\right]
\]</span></p>
<h4 id="最大似然估计maximum-likelihood-estimation"><a
target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/4967925?fr=aladdin">最大似然估计</a>（<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Maximum
Likelihood Estimation</a>）</h4>
<p>与线性回归直接输出连续的结果不同，对于分类问题，我们期望能得到一个函数
<span class="math inline">\(h_\boldsymbol\theta(\boldsymbol{x})\)</span>
来表示特征 <span class="math inline">\(\boldsymbol{x}\)</span>
下结果为正类的概率，即 <span
class="math inline">\(h_\boldsymbol\theta(\boldsymbol{x}) =
P\{Y=1|\boldsymbol{x}\}\)</span>。自然地，若 <span
class="math inline">\(h_\boldsymbol\theta(x) &gt; 0.5\)</span>
我们倾向于认为结果为正类，反之倾向于负类。</p>
<p>结合实例，这意味着我们输入病人医学图像的特征 <span
class="math inline">\(x\)</span>，便可以给出该病人肿瘤为恶性的概率(将恶性划分为正类)，若该值大于
<span class="math inline">\(0.5\)</span>
则倾向于认为肿瘤为恶性，反之认为良性。</p>
<p>由于二分类问题的结果非此即彼，所以应有：</p>
<p><span class="math display">\[
P\{Y=1|\boldsymbol{x}\} = h_\boldsymbol\theta(\boldsymbol{x})
\]</span> <span class="math display">\[
P\{Y=0|\boldsymbol{x}\} = 1 - h_\boldsymbol\theta(\boldsymbol{x})
\]</span></p>
<p>上式等价于： <span class="math display">\[
P\{Y=y|\boldsymbol{x}\} = yh_\boldsymbol\theta(\boldsymbol{x}) +
[1-y](1-h_\boldsymbol\theta(\boldsymbol{x})), \ y\in\{0,1\}
\]</span></p>
<p>对于 <span class="math inline">\(m\)</span> 个训练样本 <span
class="math inline">\(\{\boldsymbol{x}^{(j)}, y^{(j)}\},\
j=1,2,3,\cdots,m\)</span>，将其视为随机事件，则每一个样本的出现的概率为:
<span class="math display">\[
P\{Y=y^{(j)}|\boldsymbol{x}^{(j)}\} =
y^{(j)}h_\boldsymbol\theta(\boldsymbol{x}^{(j)})+(1-y^{(j)})[1-h_\boldsymbol\theta(\boldsymbol{x}^{(j)})],\
y^{(j)}\in\{0,1\}
\]</span></p>
<p>由于每个样本相互独立，则出现该样本的概率为： <span
class="math display">\[
\prod_{j=1}^m P\{Y=y^{(j)}|\boldsymbol{x}^{(j)}\},\ y^{(j)} \in \{0, 1\}
\]</span> 即： <span class="math display">\[
\prod_{j=1}^m \left\{ y^{(j)}h_\boldsymbol\theta(\boldsymbol{x}^{(j)}) +
(1-y^{(j)})[1-h_\boldsymbol\theta(\boldsymbol{x}^{(j)})] \right\},\
y^{(j)} \in \{0,1\}
\]</span></p>
<p>至此我们得到一个关于 <span
class="math inline">\(\boldsymbol\theta\)</span> 的函数，记为 <span
class="math inline">\(L(\boldsymbol\theta)\)</span>，该函数值代表了出现该训练样本的概率。最大似然估法计考虑这样一个问题，为什么会出现这样的样本分布而不是其他的分布呢？我们只能认为出现这样的分布概率是大于其他分布的，也就是说只有出现该样本分布的概率
<span class="math inline">\(L(\boldsymbol\theta)\)</span>
尽可能的大我们才得到了这个样本分布。因此我们只需求得 <span
class="math inline">\(j(\boldsymbol\theta)\)</span> 取得最大值所对应的
<span class="math inline">\(\hat{\boldsymbol\theta}\)</span>，便是我们对
<span class="math inline">\(h_\boldsymbol\theta(\boldsymbol{x})\)</span>
参数值 <span class="math inline">\(\boldsymbol\theta\)</span>
合理估计。这便是最大似然参数估计法的思想。</p>
<p>至此，我们可将代价函数看作 <span
class="math inline">\(-L(\boldsymbol\theta)\)</span>，我们只有使代价函数尽可能的小，即
<span class="math inline">\(L(\boldsymbol\theta)\)</span>
尽可能的大才能得到合理的 <span
class="math inline">\(\boldsymbol\theta\)</span>。而 <span
class="math inline">\(-L(\boldsymbol\theta)\)</span> 中的 <span
class="math inline">\(h_\boldsymbol\theta(\boldsymbol{x})\)</span>
还不明确，我们需要明确其形式。</p>
<h4 id="确定-h_boldsymbolthetaboldsymbolx-的形式">确定 <span
class="math inline">\(h_\boldsymbol\theta(\boldsymbol{x})\)</span>
的形式</h4>
<p>显然线性回归中的假设函数 <span
class="math inline">\(h_\boldsymbol\theta(\boldsymbol{x}) =
\boldsymbol\theta_0 + \sum\limits_{i=1}^n\boldsymbol\theta_ix_i\)</span>
的形式不满足分类问题，因为在分类问题中我们期望把得到的 <span
class="math inline">\(h_\boldsymbol\theta(\boldsymbol{x})\)</span>
看作是一个概率，所以其值应该是在区间 <span
class="math inline">\([0,1]\)</span>
内的。为此，我们可以对原函数进行“挤压”，使其值域属于 <span
class="math inline">\((0,1)\)</span>。此时便用到了 <a
target="_blank" rel="noopener" href="https://baike.baidu.com/item/S%E5%9E%8B%E5%87%BD%E6%95%B0/19178062?fr=aladdin">S
型函数</a>（<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid
function</a>）: <span class="math display">\[
sigmod(z) = \frac{1}{1+e^{-z}},\ z \in \mathbf{R}
\]</span></p>
<p>所以令 <span class="math display">\[
h_\boldsymbol\theta(\boldsymbol{x}) = \frac{1}{1 + e^{-z}},\
z = \boldsymbol\theta_0 + \sum\limits_{i=1}^n\boldsymbol\theta_ix_i
\]</span></p>
<p>其中 <span class="math inline">\(z\)</span> 也可记为矩阵形式： <span
class="math display">\[
\ z = \boldsymbol{\theta}^T \left[ \begin{matrix}1 \\ \boldsymbol{x}
\end{matrix} \right]
\]</span></p>
<p>至此我们就得到了 <span
class="math inline">\(h_\boldsymbol\theta(\boldsymbol{x})\)</span>
的表达式。</p>
<h4 id="整理得到代价函数-jboldsymboltheta">整理得到代价函数 <span
class="math inline">\(J(\boldsymbol\theta)\)</span></h4>
<p>确定了 <span
class="math inline">\(h_\boldsymbol\theta(\boldsymbol{x})\)</span>
形式后，<span class="math inline">\(L(\boldsymbol\theta)\)</span>
的形式完全确定，在给定训练数据下是一个关于 <span
class="math inline">\(\boldsymbol\theta\)</span> 的函数。我们可以使用
<span class="math inline">\(-L(\boldsymbol\theta)\)</span>
作为代价函数，但在最大似然估计法中更常用的方法是对 <span
class="math inline">\(L(\boldsymbol\theta)\)</span> 取对数化简（<span
class="math inline">\(\ln\)</span>
函数的单调性保证了化简前后同时取得最值），令 <span
class="math display">\[
J(\boldsymbol\theta) = -\frac1{m}\ln L(\boldsymbol\theta)
\]</span></p>
<p>进一步得到： <span class="math display">\[
J(\boldsymbol\theta) =
\begin{cases}
-\frac1{m}\sum\limits_{j=1}^m
\ln[h_\boldsymbol\theta(\boldsymbol{x}^{(j)})], &amp; y^{(j)} = 1.\\
-\frac1{m}\sum\limits_{j=1}^m
\ln[1-h_\boldsymbol\theta(\boldsymbol{x}^{(j)})], &amp; y^{(j)}=0.
\end{cases}
\]</span></p>
<p>同样可合并，写为: <span class="math display">\[
J(\boldsymbol\theta) =
-\frac1{m}\sum_{j=1}^m\Big(y^{(j)}\ln[h_\boldsymbol\theta(\boldsymbol{x}^{(j)})]
+ (1-y^{(j)})\ln[1-h_\boldsymbol\theta(\boldsymbol{x}^{(j)})]\Big)
\]</span></p>
<p>至此便得到了代价函数 <span
class="math inline">\(J(\boldsymbol\theta)\)</span>
的表达式，接下来就是最小化代价函数以得到合适的参数向量 <span
class="math inline">\(\boldsymbol{\theta}\)</span></p>
<h3 id="梯度下降">梯度下降</h3>
<p>梯度下降的核心在于求得代价函数的梯度 <span
class="math inline">\(\mathbf{Grad}J(\boldsymbol\theta)\)</span>，由高等数学知识可知:
<span class="math display">\[
\mathbf{Grad}J(\boldsymbol\theta) =
(\frac{\partial{J}}{\partial\boldsymbol{\theta}_0},
\frac{\partial{J}}{\partial\boldsymbol{\theta}_1}, \cdots,
\frac{\partial{J}}{\partial\boldsymbol{\theta}_n})
\]</span></p>
<p>下面求 <span class="math inline">\(\frac{\partial
J}{\partial\boldsymbol{\theta}_i}\)</span>：</p>
<p>先将 <span class="math inline">\(h_\boldsymbol\theta(\boldsymbol{x})
= \frac1{1+e^{-z}}\)</span> 带入 <span
class="math inline">\(J(\boldsymbol\theta)\)</span>，并化简，可得：</p>
<p><span class="math display">\[
J(\boldsymbol\theta) =
\frac1{m}\sum\limits_{j=1}^m\left[\ln(1+e^{z^{(j)}}) -
y^{(j)}z^{(j)}\right],\ z^{(j)} = \boldsymbol{\theta}^T\left[
\begin{matrix} 1 \\ \boldsymbol{x}^{(j)} \end{matrix} \right]
\]</span></p>
<p>则有：</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial{J}}{\partial\boldsymbol\theta_i} &amp;= \frac{\partial
J}{\partial z} \cdot \frac{\partial z}{\partial\boldsymbol\theta_i} \\
&amp;=
\frac1{m}\sum\limits_{j=1}^m\Big(\frac{e^{z^{(j)}}}{1+e^{z^{(j)}}}-y^{(j)}\Big)x^{(j)}*i
\\
&amp;= \frac1{m}\sum*{j=1}^{m}\left[h_\boldsymbol\theta(x^{(j)}) -
y^{(j)}\right]x_i^{(j)}
\end{aligned}
\]</span></p>
<p>至此我们就求出了代价函数对于每个参数 <span
class="math inline">\(\boldsymbol\theta_i\)</span>
的偏导，我们就可以使用梯度下降法来求得使代价函数足够小时的所有参数了：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">// 选取合适的下降率 alpha<br>repeat <span class="hljs-keyword">until</span> converge &#123;<br>  theta0 := theta0 - alpha * grad0;<br>  theta1 := theta1 - alpha * grad1;<br>  ...<br>  thetan := thetan - alpha * gradn;<br>&#125;<br></code></pre></td></tr></table></figure>
<p>在很多场景下使用矩阵运算往往具有更高的效率，下面根据 <span
class="math inline">\(\frac{\partial
J}{\partial\boldsymbol\theta_i}\)</span> 的计算公式可得到 <span
class="math inline">\(\frac{\partial
J}{\partial\boldsymbol{\theta}}\)</span> 的矩阵表示法：</p>
<p>首先记: <span class="math display">\[
\boldsymbol{X} = \left[
  \begin{matrix}
    1 &amp; [x^{(1)}]^T \\
    1 &amp; [x^{(2)}]^T \\
    \vdots &amp; \vdots \\
    1 &amp; [x^{(m)}]^T \\
  \end{matrix}
  \right]
\]</span></p>
<p>则有： <span class="math display">\[
\frac{\partial J}{\partial\boldsymbol{\theta}}
=
\frac1m\boldsymbol{X}^T\left[h_\boldsymbol\theta(X\times\boldsymbol{\theta})
- y\right]
\]</span></p>

  </article>
</div>

      
  
    
    <div class="comments gitalk-container"></div>
  


      <div class="foot">
  <div class="foot__main">
    <div class="foot-item foot-sponsers">
      
        <img class="foot-sponser" height="60px" src=/images/sp-CSC.jpg>
      
        <img class="foot-sponser" height="60px" src=/images/sp-SJTU.png>
      
        <img class="foot-sponser" height="60px" src=/images/sp-IBME_oxford.png>
      
        <img class="foot-sponser" height="60px" src=/images/sp-CompBioMed.png>
      
    </div>
    <div class="foot-item">
      <div class="foot-item__head">Links</div>
      <div class="foot-item__body">
        
          <a class="foot-item-line" href=http://www.sdspeople.fudan.edu.cn/zhuangxiahai/>Professor Xiahai Zhuang</a>
        
          <a class="foot-item-line" href=https://www.kcl.ac.uk/people/julia-a-schnabel>Professor Julia A Schnabel</a>
        
          <a class="foot-item-line" href=https://eng.ox.ac.uk/people/vicente-grau-colomer/>Professor Vicente Grau</a>
        
          <a class="foot-item-line" href=https://zmiclab.github.io/index.html>Fudan ZMIC Lab</a>
        
          <a class="foot-item-line" href=https://eng.ox.ac.uk/biomedical-image-analysis/>Oxford BioMedIA</a>
        
      </div>
    </div>
  </div>
  <div class="foot-copyright">
    <div class="foot-copyright-line">Lei Li © 2023</div>
    <div class="foot-copyright-line">
    Theme | <a target="_blank" rel="noopener" href="https://github.com/hooozen/hexo-theme-academic">Hexo Academic</a>
    </div>
  </div>
</div>
    </div>
  </div>

      </div>
    </div>
    <script src="/academic/js/page.js"></script>
    <script src="/academic/js/utils.js"></script>
<script src="/academic/js/academic-boot.js"></script>

    
<script class="academic-config" data-name="gitalk" type="application/json">{"enable":true,"js":"https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js","github_id":"marie0909","repo":"marie0909.github.io","client_id":"392609484cf5d6f9875b","client_secret":"afeadfb746568b02b80c51c676a02fd832e91038","admin_user":"marie0909","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":null,"path_md5":"49c910f53c744349c16dba7de38ef3ba"}</script>
<script src="/academic/js/third-party/gitalk.js"></script>

  </body>
</html>